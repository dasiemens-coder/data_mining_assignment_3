{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mining Assignment #3\n",
    "\n",
    "#### Group 27: Max Beinhauer, Davis Siemens\n",
    "#### Dataset: https://snap.stanford.edu/data/ego-Twitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql import Row\n",
    "from collections import defaultdict\n",
    "\n",
    "FILE_PATH = \"data/twitter_combined.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark stop in case error occurs during execution\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks on graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/Users/davis/VSCode/Data%20Mining/data_mining_assignment_3/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/davis/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/davis/.ivy2.5.2/jars\n",
      "io.graphframes#graphframes-spark4_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0fab54fa-1a31-48e9-aed3-8c4b72c862c5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.graphframes#graphframes-spark4_2.13;0.10.0 in central\n",
      "\tfound io.graphframes#graphframes-graphx-spark4_2.13;0.10.0 in central\n",
      ":: resolution report :: resolve 109ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.graphframes#graphframes-graphx-spark4_2.13;0.10.0 from central in [default]\n",
      "\tio.graphframes#graphframes-spark4_2.13;0.10.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0fab54fa-1a31-48e9-aed3-8c4b72c862c5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "25/11/19 13:55:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample lines from the graph file:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['214328887 34428380', '17116707 28465635', '380580781 18996905', '221036078 153460275', '107830991 17868918']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct undirected edges: 1342310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Vertices: 81306 - Actual Vertices:  81306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: 1342310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Assignment2\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"io.graphframes:graphframes-spark4_2.13:0.10.0\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "# Read the graph file\n",
    "graph_rdd = spark.sparkContext.textFile(FILE_PATH)\n",
    "\n",
    "# Show first few lines\n",
    "print(\"Sample lines from the graph file:\")\n",
    "print(graph_rdd.take(5))\n",
    "\n",
    "# Create edges RDD (u, v)\n",
    "edges = graph_rdd.map(lambda line: tuple(map(int, line.split()))) \n",
    "\n",
    "# Make edges undirected by sorting endpoints, e.g. (3, 10) and (10, 3) -> (3, 10)\n",
    "undirected_edges = edges.map(lambda e: (min(e[0], e[1]), max(e[0], e[1])))\n",
    "\n",
    "# Remove duplicate undirected edges\n",
    "undirected_edges_df = undirected_edges.distinct().map(\n",
    "    lambda e: Row(src=e[0], dst=e[1])\n",
    ")\n",
    "\n",
    "print(\"Distinct undirected edges:\", undirected_edges_df.count())\n",
    "\n",
    "# Create vertices DataFrame\n",
    "vertices = (\n",
    "    undirected_edges_df\n",
    "    .flatMap(lambda edge: edge)\n",
    "    .distinct()\n",
    "    .map(lambda vid: Row(id=vid))\n",
    "    .toDF()\n",
    ")\n",
    "\n",
    "# Create edges DataFrame\n",
    "edges_df = undirected_edges_df.map(lambda e: Row(src=e[0], dst=e[1])).toDF()\n",
    "\n",
    "# Build GraphFrame\n",
    "graph = GraphFrame(vertices, edges_df)\n",
    "\n",
    "# Show stats\n",
    "\n",
    "print(\"Expected Vertices: 81306 - Actual Vertices: \", graph.vertices.count())\n",
    "print(\"Edges:\", graph.edges.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Reservoir Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iimpleemnt reservoir sampling\n",
    "def reservoir_sample_rdd(rdd, k):\n",
    "    \"\"\"\n",
    "    Reservoir sampling of size k over an RDD.\n",
    "    Iterates once over the RDD using toLocalIterator.\n",
    "    \"\"\"\n",
    "    sample = []\n",
    "    t = 0\n",
    "    # Stores only one partition in memory at a time\n",
    "    # Run time: O(n) \n",
    "    # Memory: O(k) \n",
    "    for item in rdd.toLocalIterator():\n",
    "        t += 1\n",
    "        if t <= k:\n",
    "            sample.append(item)\n",
    "        else:\n",
    "            # P(j <= k) = k / t  \n",
    "            j = random.randint(1, t)\n",
    "            if j <= k:\n",
    "                sample[j - 1] = item\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir Sample of size 100:\n",
      "Edge ID: 1024097, Vertices: (321436616, 166405793)\n",
      "Edge ID: 204423, Vertices: (477088040, 382641626)\n",
      "Edge ID: 1720076, Vertices: (18753000, 43355400)\n",
      "Edge ID: 1160398, Vertices: (220729949, 221582075)\n",
      "Edge ID: 1247865, Vertices: (350322420, 92319025)\n"
     ]
    }
   ],
   "source": [
    "# Test reservoir sampling\n",
    "\n",
    "# Read the graph file again\n",
    "edge_rdd = spark.sparkContext.textFile(FILE_PATH).map(lambda line: tuple(map(int, line.split())))\n",
    "# Assign a unique ID to each edge to verify sampling correctness\n",
    "edge_rdd = edge_rdd.zipWithIndex().map(lambda x: (x[1], x[0][0], x[0][1]))\n",
    "\n",
    "k = 100\n",
    "\n",
    "reservoir_sample = reservoir_sample_rdd(edge_rdd, k)\n",
    "print(f\"Reservoir Sample of size {k}:\")\n",
    "for edge in reservoir_sample[:5]:\n",
    "    # Print first 5 sampled edges and their IDs\n",
    "    print(f\"Edge ID: {edge[0]}, Vertices: ({edge[1]}, {edge[2]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: TRIÈST\n",
    "\n",
    "This code implements TRIÈST-IMPR framework. For more details, refer to the [paper](https://www.kdd.org/kdd2016/papers/files/rfp0465-de-stefaniA.pdf).\n",
    "\n",
    "Reference:\n",
    "P. Parchas, F. Petroni, A. Rebai, F. Silvestri, and M. Vassilvitskii, \n",
    "\"TRIÈST: Counting Local and Global Triangles in Fully-Dynamic Streams with Fixed Memory Size,\" \n",
    "in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriestImpr:\n",
    "\n",
    "    def __init__(self, M):\n",
    "        self.M = M                      # reservoir size\n",
    "        self.t = 0                      # number of processed edges\n",
    "        self.S = []                     # reservoir of sampled edges\n",
    "        self.neighbors = defaultdict(set)  # adjacency structure of sampled graph\n",
    "        self.tau = 0.0                  # global triangle estimate\n",
    "        self.tau_local = defaultdict(float)  # per-node triangle estimates\n",
    "\n",
    "    # Count sampled triangles for edge (u, v)\n",
    "    def count_sampled_triangles(self, u, v):\n",
    "        # use set intersection to count common neighbors\n",
    "        return len(self.neighbors[u].intersection(self.neighbors[v]))\n",
    "\n",
    "    # Update eeight factor\n",
    "    def weight(self):\n",
    "        if self.t <= self.M:\n",
    "            return 1.0\n",
    "        # weight factor\n",
    "        return ((self.t - 1) * (self.t - 2)) / (self.M * (self.M - 1))\n",
    "\n",
    "    # Update triangle counters\n",
    "    def update_counters(self, u, v, c):\n",
    "        if c == 0:\n",
    "            return\n",
    "\n",
    "        w = self.weight()\n",
    "\n",
    "        # global update\n",
    "        self.tau += w * c\n",
    "\n",
    "        # local updates\n",
    "        common = self.neighbors[u].intersection(self.neighbors[v])\n",
    "        for wnode in common:\n",
    "            self.tau_local[wnode] += w\n",
    "\n",
    "        self.tau_local[u] += w * c\n",
    "        self.tau_local[v] += w * c\n",
    "\n",
    "    # Manage lists\n",
    "    def add_edge_to_sample(self, u, v):\n",
    "        self.neighbors[u].add(v)\n",
    "        self.neighbors[v].add(u)\n",
    "\n",
    "    def remove_edge_from_sample(self, u, v):\n",
    "        self.neighbors[u].discard(v)\n",
    "        self.neighbors[v].discard(u)\n",
    "\n",
    "    # Reservoir sampling step\n",
    "    def reservoir_step(self, u, v):\n",
    "        \"\"\"\n",
    "        Insert (u, v) into sample with reservoir logic.\n",
    "        Returns True if edge was stored.\n",
    "        \"\"\"\n",
    "        if self.t <= self.M:\n",
    "            # Reservoir not full: always insert\n",
    "            self.S.append((u, v))\n",
    "            self.add_edge_to_sample(u, v)\n",
    "            return True\n",
    "\n",
    "        # Reservoir full: do probabilistic replacement\n",
    "        r = random.randint(1, self.t)\n",
    "        if r <= self.M:   # accept this new edge\n",
    "            idx = random.randint(0, self.M - 1)\n",
    "            old_u, old_v = self.S[idx]\n",
    "\n",
    "            # remove old edge\n",
    "            self.remove_edge_from_sample(old_u, old_v)\n",
    "\n",
    "            # insert new edge\n",
    "            self.S[idx] = (u, v)\n",
    "            self.add_edge_to_sample(u, v)\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    # Main process step\n",
    "    def process_edge(self, u, v):\n",
    "        \"\"\"\n",
    "        Process a single edge (u, v) in the stream.\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        # 1. Count sampled triangles for this edge\n",
    "        c = self.count_sampled_triangles(u, v)\n",
    "\n",
    "        # 2. Update global & local triangle counters\n",
    "        self.update_counters(u, v, c)\n",
    "\n",
    "        # 3. Apply reservoir sampling\n",
    "        self.reservoir_step(u, v)\n",
    "\n",
    "    # get functions\n",
    "    def get_global_estimate(self):\n",
    "        return self.tau\n",
    "\n",
    "    def get_local_estimate(self, u):\n",
    "        return self.tau_local[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated triangles: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Read the file and process edges\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "    edges = set()\n",
    "    for line in file:\n",
    "        u, v = map(int, line.split())\n",
    "        # Make edges undirected by sorting endpoints\n",
    "        edges.add((min(u, v), max(u, v)))\n",
    "\n",
    "# Initialize TRIEST with a reservoir size of 200\n",
    "triest = TriestImpr(M=200)\n",
    "\n",
    "# Process each edge\n",
    "for u, v in edges:\n",
    "    triest.process_edge(u, v)\n",
    "\n",
    "# Print the estimated number of triangles\n",
    "print(\"Estimated triangles:\", triest.get_global_estimate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges: \n",
    "- parallization, \n",
    "- Runntime (1.4M nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
