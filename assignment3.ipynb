{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mining Assignment #3\n",
    "\n",
    "#### Group 27: Max Beinhauer, Davis Siemens\n",
    "#### Dataset: https://snap.stanford.edu/data/ego-Twitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql import Row\n",
    "from collections import defaultdict\n",
    "\n",
    "FILE_PATH = \"data/twitter_combined.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark stop in case error occurs during execution\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks on graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/Users/davis/VSCode/Data%20Mining/data_mining_assignment_3/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/davis/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/davis/.ivy2.5.2/jars\n",
      "io.graphframes#graphframes-spark4_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0fab54fa-1a31-48e9-aed3-8c4b72c862c5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.graphframes#graphframes-spark4_2.13;0.10.0 in central\n",
      "\tfound io.graphframes#graphframes-graphx-spark4_2.13;0.10.0 in central\n",
      ":: resolution report :: resolve 109ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.graphframes#graphframes-graphx-spark4_2.13;0.10.0 from central in [default]\n",
      "\tio.graphframes#graphframes-spark4_2.13;0.10.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0fab54fa-1a31-48e9-aed3-8c4b72c862c5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "25/11/19 13:55:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample lines from the graph file:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['214328887 34428380', '17116707 28465635', '380580781 18996905', '221036078 153460275', '107830991 17868918']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct undirected edges: 1342310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Vertices: 81306 - Actual Vertices:  81306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: 1342310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Assignment2\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"io.graphframes:graphframes-spark4_2.13:0.10.0\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "# Read the graph file\n",
    "graph_rdd = spark.sparkContext.textFile(FILE_PATH)\n",
    "\n",
    "# Show first few lines\n",
    "print(\"Sample lines from the graph file:\")\n",
    "print(graph_rdd.take(5))\n",
    "\n",
    "# Create edges RDD (u, v)\n",
    "edges = graph_rdd.map(lambda line: tuple(map(int, line.split()))) \n",
    "\n",
    "# Make edges undirected by sorting endpoints, e.g. (3, 10) and (10, 3) -> (3, 10)\n",
    "undirected_edges = edges.map(lambda e: (min(e[0], e[1]), max(e[0], e[1])))\n",
    "\n",
    "# Remove duplicate undirected edges\n",
    "undirected_edges_df = undirected_edges.distinct().map(\n",
    "    lambda e: Row(src=e[0], dst=e[1])\n",
    ")\n",
    "\n",
    "print(\"Distinct undirected edges:\", undirected_edges_df.count())\n",
    "\n",
    "# Create vertices DataFrame\n",
    "vertices = (\n",
    "    undirected_edges_df\n",
    "    .flatMap(lambda edge: edge)\n",
    "    .distinct()\n",
    "    .map(lambda vid: Row(id=vid))\n",
    "    .toDF()\n",
    ")\n",
    "\n",
    "# Create edges DataFrame\n",
    "edges_df = undirected_edges_df.map(lambda e: Row(src=e[0], dst=e[1])).toDF()\n",
    "\n",
    "# Build GraphFrame\n",
    "graph = GraphFrame(vertices, edges_df)\n",
    "\n",
    "# Show stats\n",
    "\n",
    "print(\"Expected Vertices: 81306 - Actual Vertices: \", graph.vertices.count())\n",
    "print(\"Edges:\", graph.edges.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Reservoir Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iimpleemnt reservoir sampling\n",
    "def reservoir_sample_rdd(rdd, k):\n",
    "    \"\"\"\n",
    "    Reservoir sampling of size k over an RDD.\n",
    "    Iterates once over the RDD using toLocalIterator.\n",
    "    \"\"\"\n",
    "    sample = []\n",
    "    t = 0\n",
    "    # Stores only one partition in memory at a time\n",
    "    # Run time: O(n) \n",
    "    # Memory: O(k) \n",
    "    for item in rdd.toLocalIterator():\n",
    "        t += 1\n",
    "        if t <= k:\n",
    "            sample.append(item)\n",
    "        else:\n",
    "            # P(j <= k) = k / t  \n",
    "            j = random.randint(1, t)\n",
    "            if j <= k:\n",
    "                sample[j - 1] = item\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir Sample of size 100:\n",
      "Edge ID: 1024097, Vertices: (321436616, 166405793)\n",
      "Edge ID: 204423, Vertices: (477088040, 382641626)\n",
      "Edge ID: 1720076, Vertices: (18753000, 43355400)\n",
      "Edge ID: 1160398, Vertices: (220729949, 221582075)\n",
      "Edge ID: 1247865, Vertices: (350322420, 92319025)\n"
     ]
    }
   ],
   "source": [
    "# Test reservoir sampling\n",
    "\n",
    "# Read the graph file again\n",
    "edge_rdd = spark.sparkContext.textFile(FILE_PATH).map(lambda line: tuple(map(int, line.split())))\n",
    "# Assign a unique ID to each edge to verify sampling correctness\n",
    "edge_rdd = edge_rdd.zipWithIndex().map(lambda x: (x[1], x[0][0], x[0][1]))\n",
    "\n",
    "k = 100\n",
    "\n",
    "reservoir_sample = reservoir_sample_rdd(edge_rdd, k)\n",
    "print(f\"Reservoir Sample of size {k}:\")\n",
    "for edge in reservoir_sample[:5]:\n",
    "    # Print first 5 sampled edges and their IDs\n",
    "    print(f\"Edge ID: {edge[0]}, Vertices: ({edge[1]}, {edge[2]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: TRIÈST\n",
    "\n",
    "This code implements a function or algorithm inspired by the TRIÈST framework. For more details, refer to the [paper](https://www.kdd.org/kdd2016/papers/files/rfp0465-de-stefaniA.pdf).\n",
    "Goal fo \n",
    "\n",
    "Reference:\n",
    "P. Parchas, F. Petroni, A. Rebai, F. Silvestri, and M. Vassilvitskii, \n",
    "\"TRIÈST: Counting Local and Global Triangles in Fully-Dynamic Streams with Fixed Memory Size,\" \n",
    "in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
